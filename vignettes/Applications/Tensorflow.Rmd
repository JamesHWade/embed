---
title: "Entity Embeddings of Categorical Variables using Tensorflow"
vignette: >
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteIndexEntry{Entity Embeddings of Categorical Variables using Tensorflow}
output:
  knitr:::html_vignette:
    toc: yes
---


```{r check-for-build, echo = FALSE, results='asis'}
eval_chunks <- as.logical(Sys.getenv("local_vignette_build", FALSE))
# Change this via `Sys.setenv(local_vignette_build = "TRUE")`
if(!eval_chunks) 
  cat(
    "(These documents take a long time to create, so only the code",
    "is shown here. The full version is at",
    "[https://tidymodels.github.io/embed](https://tidymodels.github.io/embed).)"
    )
```

```{r setup, include = FALSE, eval = eval_chunks}
library(tidymodels)
library(keras)
library(AmesHousing)
library(embed)
library(ggplot2)
library(ggiraph)
options(digits = 2)
set.seed(24566)
```

The approach encodes categorical data as multiple numeric variables using a _word embedding_ approach. Originally intended as a way to take a large number of word identifiers and represent them in a smaller dimension. Good references on this are [Guo and Berkhahn (2016)](https://arxiv.org/abs/1604.06737) and Chapter 6 of [Francois and Allaire (2018)](https://www.manning.com/books/deep-learning-with-r). 

The methodology first translates the _C_ factor levels as a set of integer values then randomly allocates them to the new _D_ numeric columns. These columns are optionally connected in a neural network to an intermediate layer of hidden units. Optionally, other predictors can be added to the network in the usual way (via the `predictors` argument) that also link to the hidden layer. This implementation uses a single layer with ReLu activations. Finally, an output layer is used with either linear activation (for numeric outcomes) or softmax (for classification).  

To translate this model to a set of embeddings, the coefficients of the original embedding layer are used to represent the original factor levels. 

As an example, we use the Ames housing data where the sale price of houses are being predicted. One predictor, neighborhood, has the most factor levels of the predictors. 

```{r ames, eval = eval_chunks}
library(tidymodels)
library(AmesHousing)
ames <- make_ames()
length(levels(ames$Neighborhood))
```

The distribution of data in the neighborhood is not uniform:

```{r ames-xtab}
ggplot(ames, aes(x = Neighborhood)) + 
  geom_bar() + 
  coord_flip() + 
  xlab("") + 
  theme_bw()
```

Fo plotting later, we calculate the simple means per neighborhood:

```{r ames-means, eval = eval_chunks}
means <- 
  ames %>%
  group_by(Neighborhood) %>%
  summarise(
    mean = mean(log10(Sale_Price)),
    n = length(Sale_Price),
    lon = median(Longitude),
    lat = median(Latitude)
  )
```

We'll fit a model with 10 hidden units and 3 encoding columns:

```{r ames-linear, eval = eval_chunks}
library(embed)
tf_embed <- 
  recipe(Sale_Price ~ ., data = ames) %>%
  step_log(Sale_Price, base = 10) %>%
  # Add some other predictors that can be used by the network. We
  # preprocess them first
  step_YeoJohnson(Lot_Area, Full_Bath, Gr_Liv_Area)  %>%
  step_range(Lot_Area, Full_Bath, Gr_Liv_Area)  %>%
  step_embed(
    Neighborhood, 
    outcome = vars(Sale_Price),
    predictors = vars(Lot_Area, Full_Bath, Gr_Liv_Area),
    num_terms = 5, 
    hidden_units = 10, 
    options = embed_control(epochs = 75, validation_split = 0.2)
  ) %>% 
  prep(training = ames)

theme_set(theme_bw() + theme(legend.position = "top"))

tf_embed$steps[[4]]$history %>%
  filter(epochs > 1) %>%
  ggplot(aes(x = epochs, y = loss, col = type)) + 
  geom_line() + 
  scale_y_log10() 
```

The embeddings are obtained using the `tidy` method:

```{r linear-coefs, warning = FALSE, eval = eval_chunks}
hood_coef <- 
  tidy(tf_embed, number = 4) %>%
  dplyr::select(-terms, -id)  %>%
  dplyr::rename(Neighborhood = level) %>%
  # Make names smaller
  rename_at(vars(contains("emb")), funs(gsub("Neighborhood_", "", ., fixed = TRUE)))
hood_coef

hood_coef <- 
  hood_coef %>% 
  inner_join(means, by = "Neighborhood")
hood_coef
```


We can make a simple, interactive plot of the new features versus the outcome:

```{r}
tf_plot <- 
  hood_coef %>%
  dplyr::select(-lon, -lat) %>%
  gather(variable, value, starts_with("embed")) %>%
  # Clean up the embedding names and add a new variable as a hover-over/tool tip
  # aesthetic for the plot
  mutate(
    label = paste0(gsub("_", " ", Neighborhood), " (n=", n, ")"),
    variable = gsub("_", " ", variable)
    ) %>%
  ggplot(aes(x = value, y = mean)) + 
  geom_point_interactive(aes(size = sqrt(n), tooltip = label), alpha = .5) + 
  facet_wrap(~variable, scales = "free_x") + 
  theme_bw() + 
  theme(legend.position = "top") + 
  ylab("Mean (log scale)") + 
  xlab("Embedding")

# Convert the plot to a format that the html file can handle
ggiraph(ggobj = tf_plot)
```


However, this has induced some between-predictor correlations:

```{r linear-cor, eval = eval_chunks}
hood_coef %>% 
  dplyr::select(contains("emb")) %>% 
  cor() %>%
  round(2)
```
